In this section, we introduce the algorithm SMITLe we proposed, including optimizing the two transfer parameters $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ and theoretically analyze the performance of our algorithm.
\subsection{Optimizing $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$}
In this part, we introduce our method to estimate proper $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ that can prevent negative transfer. We use closed-form LOO error to evaluate the performance of SMITLe for multi-class classification and optimize $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ with our novel objective function to prevent negative transfer.

From above, we can see that the hyperplane for the target problem is determined by $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$. Negative transfer happens when the model inappropriately leverage over unrelated prior knowledge, i.e. set a large value to transfer parameters $\gamma_i$ and $\beta_i$.  In our case, because we have to transfer the knowledge between different datasets as well as learning a new category, mismatched data distribution can be a serious problem and we are more likely to suffer from negative transfer. However, aggressive exploiting related prior knowledge can improve the performance of the transfer model greatly. Our algorithm should able to distinguish the utility of the prior knowledge and the transfer parameters should be optimized accordingly.

As we mentioned above, an important advantage of LS-SVM over the other model is that we can get unbiased LOO error in closed form \cite{cawley2006leave}. The unbiased LOO estimation for sample $x_i$ can be written as:
\begin{equation}
{\hat Y_{i,n}} = {Y_{i,n}} - \frac{{{\alpha _{in}}}}{{\psi_{ii}^{ - 1}}}{\text{    for   }}n = 1,...,N + 1
\end{equation}
Here $\psi^{-1}$ is the inverse of matrix $\psi$ and  $\psi_{ii}^{-1}$ is its $ith$ diagonal element. 

Let us call $\xi_i$ the multi-class prediction error for example $x_i$, and $\xi_i$ can be defined as \cite{crammer2002algorithmic}:
\begin{equation}\label{eq:train_loss}
\xi_i(\gamma,\beta) = \mathop {\max }\limits_{n \in \left\lbrace 1,...,N+1 \right\rbrace } {\left[ {1 - {\varepsilon _{n{y_i}}} + {{\hat Y}_{in}}\left( {\gamma ,\beta } \right) - {{\hat Y}_{i{y_i}}}\left( {\gamma ,\beta } \right)} \right]}
\end{equation}
Where $\varepsilon _{n{y_i}}=1$ if $n=y_i$ and 0 otherwise. We have $\xi_i(\gamma,\beta)>0$ if example $x_i$ is misclassified. The intuition behind this loss function is to enforce the distance between the true class and other classes to be at least 1.

Then we define our objective function as:
\begin{equation}\label{eq:loss}
\begin{aligned}
& \textbf{min}
& & \frac{{{\lambda _1}}}{2}\sum\limits_{n = 1}^N {{{\left\| {{\gamma _n}} \right\|}^2}}  + \frac{{{\lambda _2}}}{2}\sum\limits_{n = 1}^N {{{\left\| {{\beta _n}} \right\|}^2}}  + \sum\limits_{i = 1}^l {{\xi _i}}   \\
& \textbf{s.t.}
& & 1 - {\varepsilon _{n{y_i}}} + {\hat Y_{in}}\left( {\gamma ,\beta } \right) - {\hat Y_{i{y_i}}}\left( {\gamma ,\beta } \right) \le {\xi_i};\\
& & &\lambda_1,\lambda_2 \ge 0
\end{aligned}
\end{equation}

Here $\lambda_1$ and $\lambda_2$ are two regularization parameters to prevent negative transfer. 
From the objective function above we can see that, for certain $\lambda_1$ and $\lambda_2$, when the prior knowledge is unrelated and harmful, decreasing $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ leads to smaller loss from both regularization and multi-class prediction error for target task. Moreover, we also prove that with optimal $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ from this objective function, SMITLe can actually avoid negative transfer (for more details, see Theorem \ref{th:1}). On the other hand, if the prior knowledge is related, even though, increasing $\boldsymbol{\gamma}$ and $\boldsymbol{\beta}$ leads to larger punishment, it also leads to smaller multi-class prediction error on the target problem. So the algorithm compromises between the prior and empirical knowledge. Besides, since Eq. \eqref{eq:loss} is strongly convex, we can always guarantee that SMITLe can converge at the rate of $O(\frac{\log(t)}{t})$ (see Section \ref{subsec:analysis}).


By adding a dual set of variables, one for each constraint, we get the Lagrangian of the optimization problem:
\begin{equation}\label{eq:dual}
\begin{aligned}
 \textbf{max}\qquad {}& L\left( {\gamma ,\beta ,\xi ,\eta } \right) =\\
 &\frac{{{\lambda _1}}}{2}\sum\limits_{n = 1}^N {{{\left\| {{\gamma _n}} \right\|}^2}}  + \frac{{{\lambda _2}}}{2}\sum\limits_{n = 1}^N {{{\left\| {{\beta _n}} \right\|}^2}}  + \sum\limits_{i = 1}^l {{\xi _i}} \\
   &+ \sum\limits_{i,n} {{\eta _{i,n}}\left[ {1 - {\varepsilon _{n{y_i}}} + {{\hat Y}_{in}}\left( {\gamma ,\beta } \right) - {{\hat Y}_{i{y_i}}}\left( {\gamma ,\beta } \right) - {\xi _i}} \right]}  \\
 \textbf{s.t.} \qquad {} & \forall i,n \quad {} {\eta _{i,n}} \ge 0
\end{aligned}
\end{equation}

The problem of Eq. \eqref{eq:dual} is a non-differentiable strongly convex problem. The sub-gradient of it can be written as:
\begin{equation*}
{\Delta _\gamma }=\begin{cases}
\boldsymbol{0}&{y_i}=n\\
\left[ {0,..,\frac{{\alpha ''}_{in}}{\psi _{ii}^{ - 1}},.., - \frac{{\alpha ''}_{i{y_i}}}{\psi _{ii}^{ - 1}},..,0} \right]&{y_i},n = 1,...N\\
\left[ {0,..,\frac{{\alpha ''}_{in}}{\psi _{ii}^{ - 1}},..,0} \right]&{y_i} = N + 1;n = 1,...N\\
\left[ {0,.., - \frac{{\alpha ''}_{i{y_i}}}{\psi _{ii}^{ - 1}},..,0} \right]&\text{otherwise}
\end{cases}
\end{equation*}
\begin{equation*}
{\Delta _\beta }=\begin{cases}
 - \sum {{{\alpha ''}_{ik}}{\beta _k}} &{y_i} = N + 1;n = 1,...N\\
 \sum {{{\alpha ''}_{ik}}{\beta _k}} &{y_i} = 1,..N;n = N+1\\
\boldsymbol{0}&\text{otherwise}\\
\end{cases}
\end{equation*}
To obtain the optimal values for the problem above, we introduce our method using sub-gradient descent \cite{BoydCO} and summarize it in Alg. \ref{alg:1}. 
\input{agl1.tex}

\subsection{Analysis}\label{subsec:analysis}
In this part, we mainly discuss SMITLe in two aspects: convergence analysis and theoretical proof of preventing negative transfer.

Because $\xi_i(\gamma,\beta)$ is a convex loss function, the primal problem \eqref{eq:loss} becomes the strongly convex problem by adding the L2 regularization terms. Optimizing the strongly convex problem can lead to the following error bound:
\input{theorem2.tex}

\input{theorem.tex}
When setting $\gamma=\beta = \mathbf{0}$, we don't utilize any knowledge from previous task (see Eq. \eqref{eq:opt}).
This suggests that the training error of SMITLe won't be greater than no transfer method. 
According to statistic learning theory, we can conclude that, in any case, the superior bound of the risk for SMITLe is the risk of no transfer method, which proves SMITLe can avoid negative transfer.
