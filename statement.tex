Our method works in the following scenario. There is a image dataset (source data) containing $N$ categories and a classifier trained from this dataset to distinguish these $N$ categories. This (source) classifier and the features used to learn it is publicly accessible while the dataset itself is private (unknown distribution).  Now we collect our own image dataset (target data) coming from $N+1$ categories. This target dataset consists of $N$ identical categories to the source data and one new category related to the previous $N$ categories. In order to train a new classifier for our new task, we would expect our classifier to get better results with respect to 
\begin{itemize}
\item Maximize positive transfer. Since we know that these two task share some information, our classifier should transfer useful information as much as possible.
\item Minimize negative transfer. The data distribution of the source data is unknown. In the extreme situation, the data distribution of the two task could be totally different. Then the knowledge from previous task is negative transfer and should be disposed.
\end{itemize}

\hl{In this paper, we focus our work on transferring the knowledge with LS-SVM as the classifier for multi-class transfer problem. In the following we briefly introduce the mathermatical setting of our problem and show   }

\subsection{LS-SVM Setting and Definition}
Here we introduce the notations used in the rest of the paper. \hl{We use any letter with apostrophe to denote the information from the source data, e.g. if $f(x)$ denotes the model for the target task, $f'(x)$ denotes the model for the source one.}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{useful notations in this paper}
    \begin{tabular}{|c|L{7cm}|}
    \hline
    $f'(x)$ & binary function for source task \\
    \hline
    $f(x)$  & binary function for target task \\
    \hline
    $\phi(x)$ &  function mapping the input sample into a high dimensional feature space. \\ \hline
    $K(x,x)$ & kernel matrix with  $\phi(x_i) \cdot\phi(x_j)$ corresponding to its element $(i,j)$\\ \hline
    $X$     & instance matrix with each row representing one instance \\\hline
    $W $    & (N+1)-column hyperplane matrix for target task. Each column represents one hyperplane of a binary model \\\hline
    $W'$    & hyperplane matrix for the source task \\\hline
    $a' $   & the Lagrangian multiplier matrix for source problem. Each column represents a set of  \\\hline
    $a $    & the Lagrangian multiplier matrix for target problem \\
    \hline
    $b',b$  & the bias vector for source and target task \\
        \hline
    $a_i,w_i$ & $i_{th}$ column of matrix $a$ and $w$\\\hline
    $d_\gamma$ &  diagonal matrix with$\left[ {{\gamma _1},...,{\gamma _N}} \right]$ in its main diagonal\\\hline
    $\beta$ & row vector $\left[ {{\beta _1},...,{\beta _N}} \right]$\\ \hline
    $\varepsilon_{ny_i}$&$\varepsilon _{n{y_i}}=1$ if $n=y_i$ and 0 otherwise\\ \hline
    \end{tabular}%
  \label{tab:notation}%
\end{table}%

 

Assume that, for our $(N+1)$-category target task, ${x} \in \mathcal{X}$ and ${y} \in \mathcal{Y}=\left\{1,2,...,N+1\right\}$ are the input vector and output for the learning task respectively. Meanwhile, we have a set of binary linear classifiers $f'_n(x)=\phi(x)w_n'+b_n'$, for $n=1,...,N$ trained from an unknown distribution with One-Versus-All (OVA) strategy.  Now we want to learn a set of new classifier $f_n(x)=\phi(x)w_n+b_n, n=1,...,N+1$, so that example $x$ is assigned to the category $j$ if $j \equiv \arg {\max _{n = 1,...,N+1}}\left\{{f_n}(x)\right\}$. In LS-SVM, the solution of the model parameters $(w_n,b_n)$ can be found by solving the following optimization problem:
\begin{equation*}
\begin{aligned}
\textbf{min} && R({w_n}) + \frac{C}{2}\sum\limits_i^l {({Y_{i,n}} - \phi ({x_i}){w_n} - {b_n})^2} \\
\end{aligned}
\end{equation*}
Where $R({w_n})$ is the regularization term to guarantee good generalization performance and avoid overfitting. $\mathbf{Y}$ is a encoded label matrix so that $Y_{i,n}=1$ if $y_i=n$ and $-1$ otherwise.  

In classic LS-SVM setting, the regularization term is set to $\frac{1}{2}\left\|w_n\right\|^2$ and the optimal $w_n=\phi(X)^T\alpha_n$ while the parameters $(\aleph_n,b_n)$ can be found by solving
\begin{equation}\label{eq:linear}
\left[ {\begin{array}{*{20}{c}}
{K(X,X) + \frac{1}{C}{\rm{I}}}&\mathbf{1}\\
\mathbf{1^T}&0
\end{array}} \right]\left( \begin{array}{l}
{\alpha _n}\\
{b_n}
\end{array} \right) = \left( \begin{array}{l}
{Y_n}\\
0
\end{array} \right)  
\end{equation}
Here $I$ is the identity matrix and $\mathbf{1}$ is a column vector with all its elements equal to 1.

Now our task can be divided into two separate part: learning the $N$ overlapped categories and the new category. 
\hl{We know that the source and target share $N$ categories.} From previous work \cite{yang2007cross}, the regularization term can be written as $\frac{1}{2}{{{\left\| {{w_n} - {\gamma _n}{{w'}_n}} \right\|}^2}}$. Here, $\gamma_n$ is the regularization parameter controlling the amount of transfer.
\hl{For the task for new category}, we can use multi-source kernel learning strategy in \cite{tommasi2014learning} 


So the multi-class transfer problem can be solved by optimizing the following objective function:
\begin{equation}\label{eq:opt}
\begin{aligned}
\textbf{min}\qquad {} & \frac{1}{2}\sum\limits_{n = 1}^N {{{\left\| {{w_n} - {\gamma _n}{{w'}_n}} \right\|}^2}}  + \frac{1}{2}{\left\| {{w_{N + 1}} - \sum\limits_{k = 1}^N {w{'_k}{\beta _k}} } \right\|^2}\\& \frac{C}{2}\sum\limits_{n = 1}^{N + 1} {\sum\limits_{i = 1}^l {e_{i,n}^2} }  \\
\textbf{subject to}\qquad {} &{e_{i,n}} = {Y_{i,n}} - \phi ({x_i}){w_n} - {b_n}
\end{aligned}
\end{equation}

The closed-form of the optimal solution to  Eq. (\ref{eq:opt}) is:
\begin{equation*}
\begin{array}{*{20}{c}}
{{w_n} = {\gamma _n}{{w'}_n} + \sum\limits_i^l {{\alpha _{in}}{\phi(x_i)}} }&{n = 1,...,N}\\
{{w_{N + 1}} = \sum\limits_k^N {{\beta _k}{{w'}_k}}  + \sum\limits_i^l {{\alpha _{i(N + 1)}}{\phi(x_i)}} }&{}
\end{array}
\end{equation*}
Here $\alpha_{ij}$ is the element $(i,j)$ in $\alpha$.

Let $\psi$ denotes the first term of left-hand side in Eq. (\ref{eq:linear}). \hl{closed form in matrix format}

\begin{equation}\label{eq:solution}
  \left[ {\begin{array}{*{20}{c}}
\alpha \\
b
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
{\alpha '}\\
{b'}
\end{array}} \right] - \left[ {\begin{array}{*{20}{c}}
{\alpha ''}\\
{b''}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{d_\gamma }&{{\beta ^T}}\\
0&0
\end{array}} \right]
\end{equation}
From Eq. (\ref{eq:solution}) we can see that, the solution of Eq. (\ref{eq:linear}) is completed once $\gamma=\left[ \gamma_1,...,\gamma_N\right] $ and $\beta$ are set.

\subsection{Optimize $\gamma$ and $\beta$}
\hl{introduce LOO error estimation}

\hl{introduce our objective function}

\hl{Let us call $\xi_i$ the loss of our multi-class prediction for example $x_i$ and $\xi_i$ can be defined as} \cite{crammer2002algorithmic}:
\begin{equation}\label{eq:train_loss}
\xi_i(\gamma,\beta) = \mathop {\max }\limits_{n \in \left\lbrace 1,...,N+1 \right\rbrace } {\left[ {1 - {\varepsilon _{n{y_i}}} + {{\hat Y}_{in}}\left( {\gamma ,\beta } \right) - {{\hat Y}_{i{y_i}}}\left( {\gamma ,\beta } \right)} \right]}
\end{equation}
Where $\varepsilon _{n{y_i}}=1$ if $n=y_i$ and 0 otherwise. $\xi_i(\gamma,\beta)>0$ if example $x_i$ is misclassified. The intuition behind this loss function is to enforce the distance between the true class and other classes to be at least 1.

And we define our objective function as:
\begin{equation}\label{loss}
\begin{aligned}
& \textbf{min}
& & \frac{{{\lambda _1}}}{2}\sum\limits_{j = 1}^N {{{\left\| {{\gamma _j}} \right\|}^2}}  + \frac{{{\lambda _2}}}{2}\sum\limits_{j = 1}^N {{{\left\| {{\beta _j}} \right\|}^2}}  + \sum\limits_{i = 1}^l {{\xi _i}}   \\
& \textbf{subject to}
& & 1 - {\varepsilon _{r{y_i}}} + {\hat Y_{ir}}\left( {\gamma ,\beta } \right) - {\hat Y_{i{y_i}}}\left( {\gamma ,\beta } \right) \le {\xi_i};\\
& & &\lambda_1,\lambda_2 \ge 0
\end{aligned}
\end{equation}
\hl{explanation of two parameters, further refer to }\cite{crammer2002algorithmic}

By adding a dual set of variables, one for each constraint, we get the Lagrangian of the optimization problem:
\begin{equation}\label{eq:dual}
\begin{aligned}
 \textbf{max}\qquad {}& L\left( {\gamma ,\beta ,\xi ,\eta } \right) =\frac{{{\lambda _1}}}{2}\sum\limits_{j = 1}^N {{{\left\| {{\gamma _j}} \right\|}^2}}  + \frac{{{\lambda _2}}}{2}\sum\limits_{j = 1}^N {{{\left\| {{\beta _j}} \right\|}^2}}  + \sum\limits_{i = 1}^l {{\xi _i}} \\
   &+ \sum\limits_{i,r} {{\eta _{i,r}}\left[ {1 - {\varepsilon _{r{y_i}}} + {{\hat Y}_{ir}}\left( {\gamma ,\beta } \right) - {{\hat Y}_{i{y_i}}}\left( {\gamma ,\beta } \right) - {\xi _i}} \right]}  \\
 \textbf{subject to} \qquad {} & \forall i,r \quad {} {\eta _{i,r}} \ge 0
\end{aligned}
\end{equation}

\hl{Algorithm for optimizaiton, convex-non-differentiable}
\input{agl1.tex}

\subsection{Analysis}
\hl{Convergence analysis}

\hl{Superior bound analysis}
\input{theorem.tex}

When setting $\gamma=\beta = \mathbf{0}$, we don't utilize any knowledge from previous task (see Eq. \eqref{eq:opt}). From Theorem \ref{th:supb} we can conclude \hl{our method can always outperform the method learning directly.}

\hl{discuss $\lambda$}