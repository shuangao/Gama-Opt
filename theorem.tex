\begin{theorem}\label{th:supb}
Assume that $\bar \xi_i$ is the multi-class loss of example $x_i$ when $\gamma=\beta = \mathbf{0}$. Let $\gamma^*, \beta^*$ be the optimal solution for Eq. \eqref{eq:dual} and $\xi_i^*$ be the multi-class loss with respective to example $x_i$. Then for every example $x_i$, we have:\[\xi_i \le \bar \xi_i\]
\end{theorem}
\begin{proof}
When $\gamma=\beta = \mathbf{0}$, from Eq. \eqref{eq:train_loss} we can get:
\begin{equation*}
{\bar \xi _i} = \mathop {\max }\limits_n \left[ {1 - {\varepsilon _{n{y_i}}} + \frac{{\left( {{{\alpha '}_{i{y_i}}} - {{\alpha '}_{in}}} \right)}}{{\psi _{ii}^{ - 1}}}} \right]
\end{equation*}
To obtain the optimal value of $\gamma$ and $\beta$, we have to seek the saddle point of the Lagrangian problem in \eqref{eq:dual} by finding the minimum for the prime variables $\left\{ \gamma, \beta, \xi \right\}$ and the maximum for the dual variables $\eta $. To find the minimum of the primal problem, we require:
\begin{equation*}
\frac{{\partial L}}{{\partial {\xi _i}}} = 1 - \sum\limits_n {{\eta _{in}}}  = 0 \to \sum\limits_n {{\eta _{in}}}  = 1
\end{equation*}   
Similarly, for $\gamma$ and $\beta$, we require:
\begin{eqnarray}
\frac{{\partial L}}{{\partial {\gamma _n}}} &=& {\lambda _1}{\gamma _n} + \sum\limits_i {{\eta _{in}}{\theta _{in}}}  - \sum\limits_{i,n = {y_i}} {\left( {\sum\limits_q {{\eta _{iq}}} } \right){\theta _{in}}{\gamma _n}}  \nonumber\\
&=_1 &{\lambda _1}{\gamma _n} + \sum\limits_i {{\eta _{in}}{\theta _{in}}}  - \sum\limits_i {{\varepsilon _{n{y_i}}}{\theta _{in}}}  = 0  \nonumber\\
&\Rightarrow & \gamma _n^* = \frac{1}{{{\lambda _1}}}\sum\limits_i {\left( {{\varepsilon _{n{y_i}}} - {\eta _{in}}} \right){\theta _{in}}} 
\end{eqnarray}
In $=_1$ we use the facts that $\sum_n\eta_{in}=1$ and use $\varepsilon_{ny_i}$ to replace it.
\begin{eqnarray}
\frac{{\partial L}}{{\partial {\beta _n}}} &=& {\lambda _2}{\beta _n} + \left[ {\sum\limits_{i,n} {\frac{{{\eta _{in}}{{\alpha ''}_{in}}}}{{\psi_{ii}^{ - 1}}}\left( {{\delta _n} - {\delta _{{y_i}}}} \right)} } \right] = 0 \nonumber \\
&\Rightarrow &\beta _n^* = \frac{1}{{{\lambda _2}}}\sum\limits_{i,n} {\frac{{{\eta _{in}}{{\alpha ''}_{in}}}}{{\psi _{ii}^{ - 1}}}\left( {{\delta _{{y_i}}} - {\delta _n}} \right)} 
\end{eqnarray}
substituting Eq. \eqref{eq:train_loss} with $\gamma^*$ and $\beta^*$, we get:
\begin{eqnarray*}
 {\xi_i^*}\left( {{\gamma ^*},{\beta ^*}} \right) &  =& \mathop {\max }\limits_n \left[ {1 - {\varepsilon _{n{y_i}}} + {{\hat Y}_{in}}\left( {{\gamma ^*},{\beta ^*}} \right) - {{\hat Y}_{i{y_i}}}\left( {{\gamma ^*},{\beta ^*}} \right)} \right] \\
   &  =& \mathop {\max }\limits_n \Big [ 1 - {\varepsilon _{n{y_i}}} + \frac{{\left( {{{\alpha '}_{i{y_i}}} - {{\alpha '}_{in}}} \right)}}{{\psi_{ii}^{ - 1}}} \\
   &&\qquad {}- {\lambda _1}{{\left\| {{\gamma_n ^*}} \right\|}^2} - {\lambda _2}{{\left\| {{\beta_n ^*}} \right\|}^2} \Big ]\\
   & \le& \mathop {\max }\limits_n \left[ {1 - {\varepsilon _{n{y_i}}} + \frac{{\left( {{{\alpha '}_{i{y_i}}} - {{\alpha '}_{in}}} \right)}}{{\psi _{ii}^{ - 1}}}} \right] =\bar \xi_i
\end{eqnarray*}

\end{proof}